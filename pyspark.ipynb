{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('ss').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv('anomaly_det_dashboard_shopper_conv.csv',header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get columns\n",
    "df = data.selectExpr(\"totalshoppertraffic_visitors as visitors\", \"digital_orders as orders\", \"cust_prospect_ind as customer\", \"visit_device_type as device\", \"event_dt as date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'from_unixtime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1ca9057545b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m df = df.select('visitors', 'orders', 'customer', 'device',\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mfrom_unixtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munix_timestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MM/dd/y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'from_unixtime' is not defined"
     ]
    }
   ],
   "source": [
    "df = df.select('visitors', 'orders', 'customer', 'device',\n",
    "    from_unixtime(unix_timestamp('date', 'MM/dd/y')).alias('date')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.orderBy(df.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "df.date = df.select(to_date(df.date,'MM/dd/yy').alias('date')).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------------+--------------+-------+\n",
      "|visitors|orders|    customer|        device|   date|\n",
      "+--------+------+------------+--------------+-------+\n",
      "|  360930|  1024|UNDETERMINED|   All Devices|3/12/19|\n",
      "|   13030|   291|    CUSTOMER|        Tablet|3/12/19|\n",
      "|     927|     0|UNDETERMINED|Gaming Console|3/12/19|\n",
      "|  251155|  4879|    CUSTOMER|   All Devices|3/12/19|\n",
      "|  161097|  3820|    CUSTOMER|       Desktop|3/12/19|\n",
      "|   21400|    90|    PROSPECT|  Mobile Phone|3/12/19|\n",
      "|  270573|  1161|All Visitors|  Mobile Phone|3/12/19|\n",
      "|      16|     1|    CUSTOMER|Gaming Console|3/12/19|\n",
      "|  352032|  4574|All Visitors|       Desktop|3/12/19|\n",
      "|    3107|    14|    PROSPECT|        Tablet|3/12/19|\n",
      "|   33368|   374|All Visitors|        Tablet|3/12/19|\n",
      "|   45624|   207|    PROSPECT|   All Devices|3/12/19|\n",
      "|  657024|  6110|All Visitors|   All Devices|3/12/19|\n",
      "|   20966|   103|    PROSPECT|       Desktop|3/12/19|\n",
      "|     163|     0|    PROSPECT|Gaming Console|3/12/19|\n",
      "|  172380|   304|UNDETERMINED|  Mobile Phone|3/12/19|\n",
      "|    1106|     1|All Visitors|Gaming Console|3/12/19|\n",
      "|  170359|   651|UNDETERMINED|       Desktop|3/12/19|\n",
      "|   77045|   767|    CUSTOMER|  Mobile Phone|3/12/19|\n",
      "|   17273|    69|UNDETERMINED|        Tablet|3/12/19|\n",
      "+--------+------+------------+--------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid argument, not a string or column: Row(date=datetime.date(2019, 3, 12)) of type <class 'pyspark.sql.types.Row'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3bd766dc07a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.4.3/libexec/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36msort\u001b[0;34m(self, *cols, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \"\"\"\n\u001b[0;32m-> 1096\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sort_cols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.4.3/libexec/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_sort_cols\u001b[0;34m(self, cols, kwargs)\u001b[0m\n\u001b[1;32m   1123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1125\u001b[0;31m         \u001b[0mjcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_to_java_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1126\u001b[0m         \u001b[0mascending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascending'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.4.3/libexec/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1125\u001b[0;31m         \u001b[0mjcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_to_java_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1126\u001b[0m         \u001b[0mascending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascending'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.4.3/libexec/python/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_java_column\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;34m\"{0} of type {1}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;34m\"For column literals, use 'lit', 'array', 'struct' or 'create_map' \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \"function.\".format(col, type(col)))\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mjcol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid argument, not a string or column: Row(date=datetime.date(2019, 3, 12)) of type <class 'pyspark.sql.types.Row'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function."
     ]
    }
   ],
   "source": [
    "df.orderBy(df.date, ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid argument, not a string or column: Row(date=datetime.date(2019, 3, 12)) of type <class 'pyspark.sql.types.Row'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-10b81299fa10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#df = df.select('visitors', 'orders', 'customer', 'device',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m  \u001b[0;31m#   from_unixtime(unix_timestamp('date', 'MM/dd/y')).alias('date'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.4.3/libexec/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36msort\u001b[0;34m(self, *cols, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \"\"\"\n\u001b[0;32m-> 1096\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sort_cols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.4.3/libexec/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_sort_cols\u001b[0;34m(self, cols, kwargs)\u001b[0m\n\u001b[1;32m   1123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1125\u001b[0;31m         \u001b[0mjcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_to_java_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1126\u001b[0m         \u001b[0mascending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascending'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.4.3/libexec/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1125\u001b[0;31m         \u001b[0mjcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_to_java_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1126\u001b[0m         \u001b[0mascending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascending'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.4.3/libexec/python/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_java_column\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;34m\"{0} of type {1}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;34m\"For column literals, use 'lit', 'array', 'struct' or 'create_map' \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \"function.\".format(col, type(col)))\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mjcol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid argument, not a string or column: Row(date=datetime.date(2019, 3, 12)) of type <class 'pyspark.sql.types.Row'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function."
     ]
    }
   ],
   "source": [
    "#df = df.select('visitors', 'orders', 'customer', 'device',\n",
    " #   from_unixtime(unix_timestamp('date', 'MM/dd/y')).alias('date'))\n",
    "#df.orderBy(df.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_mobile = df.filter((df.customer == 'CUSTOMER') & (df.device == 'Mobile Phone'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'visitors'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cust_mobile.visitors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"\"\"\n",
    " # SELECT TO_DATE(CAST(UNIX_TIMESTAMP(event_dt, 'MM/dd/yy') AS TIMESTAMP)) from myTable AS newdate\"\"\"\n",
    "#).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sqlContext.registerDataFrameAsTable(data, 'df' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sqlContext.registerDataFrameAsTable(data, \"myTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2 = sqlContext.sql(\"SELECT totalshoppertraffic_visitors as visitors, digital_orders as orders, cust_prospect_ind as customer, visit_device_type as device, event_dt as dt from myTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- totalshoppertraffic_visitors: string (nullable = true)\n",
      " |-- totalshoppertraffic_visits: string (nullable = true)\n",
      " |-- digital_orders: string (nullable = true)\n",
      " |-- aal_orders: string (nullable = true)\n",
      " |-- eup_orders: string (nullable = true)\n",
      " |-- nao_orders: string (nullable = true)\n",
      " |-- nse_orders: string (nullable = true)\n",
      " |-- nso_orders: string (nullable = true)\n",
      " |-- nsp_orders: string (nullable = true)\n",
      " |-- npp_orders: string (nullable = true)\n",
      " |-- total_orders: string (nullable = true)\n",
      " |-- cust_prospect_ind: string (nullable = true)\n",
      " |-- visit_device_type: string (nullable = true)\n",
      " |-- event_dt: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['totalshoppertraffic_visitors',\n",
       " 'totalshoppertraffic_visits',\n",
       " 'digital_orders',\n",
       " 'aal_orders',\n",
       " 'eup_orders',\n",
       " 'nao_orders',\n",
       " 'nse_orders',\n",
       " 'nso_orders',\n",
       " 'nsp_orders',\n",
       " 'npp_orders',\n",
       " 'total_orders',\n",
       " 'cust_prospect_ind',\n",
       " 'visit_device_type',\n",
       " 'event_dt']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1211"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|cust_prospect_ind|\n",
      "+-------+-----------------+\n",
      "|  count|             1211|\n",
      "|   mean|             null|\n",
      "| stddev|             null|\n",
      "|    min|     All Visitors|\n",
      "|    max|     UNDETERMINED|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.describe('cust_prospect_ind').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[cust_prospect_ind: string]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.select('cust_prospect_ind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "289"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.filter(data.cust_prospect_ind == 'CUSTOMER').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+--------------------------+--------------+----------+----------+----------+----------+----------+----------+----------+------------+-----------------+-----------------+--------+\n",
      "|totalshoppertraffic_visitors|totalshoppertraffic_visits|digital_orders|aal_orders|eup_orders|nao_orders|nse_orders|nso_orders|nsp_orders|npp_orders|total_orders|cust_prospect_ind|visit_device_type|event_dt|\n",
      "+----------------------------+--------------------------+--------------+----------+----------+----------+----------+----------+----------+----------+------------+-----------------+-----------------+--------+\n",
      "|                       37658|                     40944|           385|        50|       240|        47|        19|        12|         6|        11|         460|     All Visitors|           Tablet| 3/21/19|\n",
      "|                       46449|                     51348|           476|        58|       314|        52|        38|         6|         2|         6|         560|     All Visitors|           Tablet| 2/24/19|\n",
      "|                      653760|                    718205|          5783|      1181|      2782|       736|       672|       244|        63|       105|        7192|     All Visitors|      All Devices| 3/21/19|\n",
      "|                      270573|                    303420|          1161|       181|       425|       155|       269|        65|        32|        34|        1870|     All Visitors|     Mobile Phone| 3/12/19|\n",
      "|                        2104|                      2175|             0|         0|         0|         0|         0|         0|         0|         0|           0|     All Visitors|   Gaming Console| 3/21/19|\n",
      "|                       33368|                     36544|           374|        46|       213|        39|        40|         9|        17|        10|         449|     All Visitors|           Tablet| 3/12/19|\n",
      "|                         906|                       929|             0|         0|         0|         0|         0|         0|         0|         0|           0|     All Visitors|   Gaming Console|  3/7/19|\n",
      "|                        1106|                      1135|             1|         0|         1|         0|         0|         0|         0|         0|           1|     All Visitors|   Gaming Console| 3/12/19|\n",
      "|                      227320|                    250100|          1049|       163|       399|       123|       258|        73|        15|        18|        2072|     All Visitors|     Mobile Phone|  3/7/19|\n",
      "|                         110|                       124|             2|         0|         1|         0|         1|         0|         0|         0|           2|     All Visitors|   Gaming Console|  2/3/19|\n",
      "|                      271604|                    293853|          4171|       752|      2375|       394|       341|        87|        33|       189|        5221|     All Visitors|          Desktop|  3/7/19|\n",
      "|                      226302|                    249483|           922|       168|       350|       112|       211|        55|        14|        12|        1206|     All Visitors|     Mobile Phone|  2/3/19|\n",
      "|                       25405|                     27283|           259|        37|       165|        24|        18|         9|         4|         2|         319|     All Visitors|           Tablet|  3/7/19|\n",
      "|                      339804|                    378223|          1233|       228|       341|       177|       325|       108|        26|        28|        1966|     All Visitors|     Mobile Phone| 3/21/19|\n",
      "|                      525185|                    572165|          5479|       952|      2939|       541|       617|       169|        52|       209|        7554|     All Visitors|      All Devices|  3/7/19|\n",
      "|                       41521|                     45406|           373|        32|       230|        49|        27|         9|        15|        11|         451|     All Visitors|           Tablet| 3/29/19|\n",
      "|                      706084|                    779600|          6060|      1128|      3013|       723|       754|       207|       105|       130|        7555|     All Visitors|      All Devices| 3/29/19|\n",
      "|                       36930|                     40006|           292|        43|       171|        38|        21|         8|         4|         7|         338|     All Visitors|           Tablet|  2/3/19|\n",
      "|                        2886|                      3022|             0|         0|         0|         0|         0|         0|         0|         0|           1|     All Visitors|   Gaming Console| 3/29/19|\n",
      "|                      657024|                    726526|          6110|      1068|      3098|       720|       689|       172|        94|       269|        7750|     All Visitors|      All Devices| 3/12/19|\n",
      "+----------------------------+--------------------------+--------------+----------+----------+----------+----------+----------+----------+----------+------------+-----------------+-----------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.orderBy(data.cust_prospect_ind, ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sqlContext.sql('select * from data').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSparkSession():\n",
    "    return pyspark.sql.SparkSession.builder.getOrCreate()\n",
    " \n",
    "def loadData(sparkSession):\n",
    "    sparkSQL = sparkSession.read.format(\"jdbc\").option(\"url\", \"jdbc:mysql://my-db-domain:3306/sfdc\").option(\"dbtable\", \"sfdc.opportunity\").option(\"user\", \"my-db-username\").option(\"password\", \"my-db-password\").load()\n",
    "    sparkSQL.createOrReplaceTempView(\"opp\")\n",
    "    return sparkSQL.sql_ctx.sql(\"SELECT DATE_FORMAT(CloseDate,'yyyy-MM') as CloseDate, SUM(Amount) as Amount FROM opp WHERE IsWon='true' AND IsClosed='true' GROUP BY DATE_FORMAT(CloseDate,'yyyy-MM') ORDER BY CloseDate\")\n",
    " \n",
    "def saveData(result):\n",
    "    result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_P = 14\n",
    "MAX_D = 14\n",
    "MAX_Q = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ARIMA_Spark(amounts, period):\n",
    "    spark_context = SparkContext.getOrCreate()\n",
    "    model = spark_context._jvm.com.cloudera.sparkts.models.ARIMA.autoFit(_py2java(spark_context, Vectors.dense(amounts)), MAX_P, MAX_D, MAX_Q)\n",
    "    p = _java2py(spark_context, model.p())\n",
    "    d = _java2py(spark_context, model.d())\n",
    "    q = _java2py(spark_context, model.q())\n",
    "    jts = _py2java(spark_context, Vectors.dense(amounts))\n",
    "    aic = model.approxAIC(jts)\n",
    "    print (\"ARIMA(\", p, d, q, \")\", \"AIC=\", aic)\n",
    "    jfore = model.forecast(jts, period)\n",
    "    return _java2py(spark_context, jfore)[len(amounts):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.version >= '3':\n",
    "    long = int\n",
    "    unicode = str\n",
    "\n",
    "import py4j.protocol\n",
    "from py4j.protocol import Py4JJavaError\n",
    "from py4j.java_gateway import JavaObject\n",
    "from py4j.java_collections import JavaArray, JavaList\n",
    "\n",
    "from pyspark import RDD, SparkContext\n",
    "from pyspark.serializers import PickleSerializer, AutoBatchedSerializer\n",
    "from pyspark.sql import DataFrame, SQLContext\n",
    "\n",
    "# Hack for support float('inf') in Py4j\n",
    "_old_smart_decode = py4j.protocol.smart_decode\n",
    "\n",
    "_float_str_mapping = {\n",
    "    'nan': 'NaN',\n",
    "    'inf': 'Infinity',\n",
    "    '-inf': '-Infinity',\n",
    "}\n",
    "\n",
    "\n",
    "def _new_smart_decode(obj):\n",
    "    if isinstance(obj, float):\n",
    "        s = str(obj)\n",
    "        return _float_str_mapping.get(s, s)\n",
    "    return _old_smart_decode(obj)\n",
    "\n",
    "py4j.protocol.smart_decode = _new_smart_decode\n",
    "\n",
    "\n",
    "_picklable_classes = [\n",
    "    'LinkedList',\n",
    "    'SparseVector',\n",
    "    'DenseVector',\n",
    "    'DenseMatrix',\n",
    "    'Rating',\n",
    "    'LabeledPoint',\n",
    "]\n",
    "\n",
    "\n",
    "# this will call the MLlib version of pythonToJava()\n",
    "def _to_java_object_rdd(rdd):\n",
    "    \"\"\" Return a JavaRDD of Object by unpickling\n",
    "\n",
    "    It will convert each Python object into Java object by Pyrolite, whenever the\n",
    "    RDD is serialized in batch or not.\n",
    "    \"\"\"\n",
    "    rdd = rdd._reserialize(AutoBatchedSerializer(PickleSerializer()))\n",
    "    return rdd.ctx._jvm.org.apache.spark.mllib.api.python.SerDe.pythonToJava(rdd._jrdd, True)\n",
    "\n",
    "\n",
    "def _py2java(sc, obj):\n",
    "    \"\"\" Convert Python object into Java \"\"\"\n",
    "    if isinstance(obj, RDD):\n",
    "        obj = _to_java_object_rdd(obj)\n",
    "    elif isinstance(obj, DataFrame):\n",
    "        obj = obj._jdf\n",
    "    elif isinstance(obj, SparkContext):\n",
    "        obj = obj._jsc\n",
    "    elif isinstance(obj, list):\n",
    "        obj = [_py2java(sc, x) for x in obj]\n",
    "    elif isinstance(obj, JavaObject):\n",
    "        pass\n",
    "    elif isinstance(obj, (int, long, float, bool, bytes, unicode)):\n",
    "        pass\n",
    "    else:\n",
    "        data = bytearray(PickleSerializer().dumps(obj))\n",
    "        obj = sc._jvm.org.apache.spark.mllib.api.python.SerDe.loads(data)\n",
    "    return obj\n",
    "\n",
    "\n",
    "def _java2py(sc, r, encoding=\"bytes\"):\n",
    "    if isinstance(r, JavaObject):\n",
    "        clsName = r.getClass().getSimpleName()\n",
    "        # convert RDD into JavaRDD\n",
    "        if clsName != 'JavaRDD' and clsName.endswith(\"RDD\"):\n",
    "            r = r.toJavaRDD()\n",
    "            clsName = 'JavaRDD'\n",
    "\n",
    "        if clsName == 'JavaRDD':\n",
    "            jrdd = sc._jvm.org.apache.spark.mllib.api.python.SerDe.javaToPython(r)\n",
    "            return RDD(jrdd, sc)\n",
    "\n",
    "        if clsName == 'Dataset':\n",
    "            return DataFrame(r, SQLContext.getOrCreate(sc))\n",
    "\n",
    "        if clsName in _picklable_classes:\n",
    "            r = sc._jvm.org.apache.spark.mllib.api.python.SerDe.dumps(r)\n",
    "        elif isinstance(r, (JavaArray, JavaList)):\n",
    "            try:\n",
    "                r = sc._jvm.org.apache.spark.mllib.api.python.SerDe.dumps(r)\n",
    "            except Py4JJavaError:\n",
    "                pass  # not pickable\n",
    "\n",
    "    if isinstance(r, (bytearray, bytes)):\n",
    "        r = PickleSerializer().loads(bytes(r), encoding=encoding)\n",
    "    return r\n",
    "\n",
    "\n",
    "def callJavaFunc(sc, func, *args):\n",
    "    \"\"\" Call Java Function \"\"\"\n",
    "    args = [_py2java(sc, a) for a in args]\n",
    "    return _java2py(sc, func(*args))\n",
    "\n",
    "\n",
    "def callMLlibFunc(name, *args):\n",
    "    \"\"\" Call API in PythonMLLibAPI \"\"\"\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    api = getattr(sc._jvm.PythonMLLibAPI(), name)\n",
    "    return callJavaFunc(sc, api, *args)\n",
    "\n",
    "\n",
    "class JavaModelWrapper(object):\n",
    "    \"\"\"\n",
    "    Wrapper for the model in JVM\n",
    "    \"\"\"\n",
    "    def __init__(self, java_model):\n",
    "        self._sc = SparkContext.getOrCreate()\n",
    "        self._java_model = java_model\n",
    "\n",
    "    def __del__(self):\n",
    "        self._sc._gateway.detach(self._java_model)\n",
    "\n",
    "    def call(self, name, *a):\n",
    "        \"\"\"Call method of java_model\"\"\"\n",
    "        return callJavaFunc(self._sc, getattr(self._java_model, name), *a)\n",
    "\n",
    "\n",
    "def inherit_doc(cls):\n",
    "    \"\"\"\n",
    "    A decorator that makes a class inherit documentation from its parents.\n",
    "    \"\"\"\n",
    "    for name, func in vars(cls).items():\n",
    "        # only inherit docstring for public functions\n",
    "        if name.startswith(\"_\"):\n",
    "            continue\n",
    "        if not func.__doc__:\n",
    "            for parent in cls.__bases__:\n",
    "                parent_func = getattr(parent, name, None)\n",
    "                if parent_func and getattr(parent_func, \"__doc__\", None):\n",
    "                    func.__doc__ = parent_func.__doc__\n",
    "                    break\n",
    "    return cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "cust_mobile = cust_mobile.withColumn(\"visitors\", cust_mobile[\"visitors\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "visitors = np.array([row for row in cust_mobile.select(cust_mobile.visitors).collect()]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(visitors=79575)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cust_mobile.select(cust_mobile.visitors).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-2445b26bde49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict_ARIMA_Spark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisitors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-87b2b706a41b>\u001b[0m in \u001b[0;36mpredict_ARIMA_Spark\u001b[0;34m(amounts, period)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_ARIMA_Spark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mspark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloudera\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mARIMA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautoFit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_P\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_Q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "predict_ARIMA_Spark(visitors,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sparkts\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/9f/6b86248f618fa0aaf0f8e48888347c6fc1c954881cabe36128935104e4fe/sparkts-0.2.0.tar.gz (45.5MB)\n",
      "\u001b[K     |████████████████████████████████| 45.5MB 5.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.13 in /Users/6jncnk4f/anaconda3/lib/python3.7/site-packages (from sparkts) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.9.2 in /Users/6jncnk4f/anaconda3/lib/python3.7/site-packages (from sparkts) (1.16.4)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /Users/6jncnk4f/anaconda3/lib/python3.7/site-packages (from pandas>=0.13->sparkts) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2011k in /Users/6jncnk4f/anaconda3/lib/python3.7/site-packages (from pandas>=0.13->sparkts) (2019.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/6jncnk4f/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.5.0->pandas>=0.13->sparkts) (1.12.0)\n",
      "Building wheels for collected packages: sparkts\n",
      "  Building wheel for sparkts (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/6jncnk4f/Library/Caches/pip/wheels/d3/57/5a/9af7c351e45adeee7e3e07e1a747fada0f56b930ec1b7acfb1\n",
      "Successfully built sparkts\n",
      "Installing collected packages: sparkts\n",
      "Successfully installed sparkts-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sparkts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparkts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkts.models import ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autofit(ts, maxp=5, maxd=2, maxq=5, sc=None):\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
